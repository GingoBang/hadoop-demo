apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "hadoop.fullname" . }}
  labels:
    app.kubernetes.io/name: {{ include "hadoop.name" . }}
    helm.sh/chart: {{ include "hadoop.chart" . }}
    app.kubernetes.io/instance: {{ .Release.Name }}
data:
  bootstrap.sh: |
    #!/bin/bash -x

    echo Starting

    : ${HADOOP_HOME:=/opt/hadoop}

    echo Using ${HADOOP_HOME} as HADOOP_HOME

    . $HADOOP_HOME/etc/hadoop/hadoop-env.sh

    # ------------------------------------------------------
    # Directory to find config artifacts
    # ------------------------------------------------------

    CONFIG_DIR="/tmp/hadoop-config"

    # ------------------------------------------------------
    # Copy config files from volume mount
    # ------------------------------------------------------

    for f in slaves core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml capacity-scheduler.xml; do
      if [[ -e ${CONFIG_DIR}/$f ]]; then
        cp -f ${CONFIG_DIR}/$f $HADOOP_HOME/etc/hadoop/$f
      else
        echo "ERROR: Could not find $f in $CONFIG_DIR"
        exit 1
      fi
    done

    # ------------------------------------------------------
    # installing libraries if any
    # (resource urls added comma separated to the ACP system variable)
    # ------------------------------------------------------
    cd $HADOOP_HOME/share/hadoop/common ; for cp in ${ACP//,/ }; do  echo == $cp; curl -LO $cp ; done; cd -

    # ------------------------------------------------------
    # Start NAMENODE
    # ------------------------------------------------------
    if [[ "${HOSTNAME}" =~ "hdfs-nn" ]]; then
      # sed command changing REPLACEME in $HADOOP_HOME/etc/hadoop/hdfs-site.xml to actual port numbers
      sed -i "s/EXTERNAL_HTTP_PORT_REPLACEME/9864/" $HADOOP_HOME/etc/hadoop/hdfs-site.xml
      sed -i "s/EXTERNAL_DATA_PORT_REPLACEME/9866/" $HADOOP_HOME/etc/hadoop/hdfs-site.xml

      mkdir -p /root/hdfs/namenode
      if [ ! -f /root/hdfs/namenode/formated ]; then
        # Only format if necessary
        $HADOOP_HOME/bin/hdfs namenode -format -force -nonInteractive && echo 1 > /root/hdfs/namenode/formated
      fi
      $HADOOP_HOME/bin/hdfs --loglevel {{ .Values.logLevel }} --daemon start namenode
    fi

    # ------------------------------------------------------
    # Start DATA NODE
    # ------------------------------------------------------
    if [[ "${HOSTNAME}" =~ "hdfs-dn" ]]; then
      # Split hostname at "-" into an array
      # Example hostname: hadoop-hadoop-hdfs-dn-0
      HOSTNAME_ARR=(${HOSTNAME//-/ })
      # Add instance number to start of external port ranges
      EXTERNAL_HTTP_PORT=$(({{ .Values.hdfs.dataNode.externalHTTPPortRangeStart }} + ${HOSTNAME_ARR[4]}))
      EXTERNAL_DATA_PORT=$(({{ .Values.hdfs.dataNode.externalDataPortRangeStart }} + ${HOSTNAME_ARR[4]}))

      # sed command changing REPLACEME in $HADOOP_HOME/etc/hadoop/hdfs-site.xml to actual port numbers
      sed -i "s/EXTERNAL_HTTP_PORT_REPLACEME/${EXTERNAL_HTTP_PORT}/" $HADOOP_HOME/etc/hadoop/hdfs-site.xml
      sed -i "s/EXTERNAL_DATA_PORT_REPLACEME/${EXTERNAL_DATA_PORT}/" $HADOOP_HOME/etc/hadoop/hdfs-site.xml

      mkdir -p /root/hdfs/datanode

      #  Wait (with timeout) for namenode
      TMP_URL="http://{{ include "hadoop.fullname" . }}-hdfs-nn:9870"
      if timeout 5m bash -c "until curl -sf $TMP_URL; do echo Waiting for $TMP_URL; sleep 5; done"; then
        $HADOOP_HOME/bin/hdfs --loglevel {{ .Values.logLevel }} --daemon start datanode
      else 
        echo "$0: Timeout waiting for $TMP_URL, exiting."
        exit 1
      fi

    fi

    # ------------------------------------------------------
    # Start RESOURCE MANAGER and PROXY SERVER as daemons
    # ------------------------------------------------------
    if [[ "${HOSTNAME}" =~ "yarn-rm" ]]; then
      $HADOOP_HOME/bin/yarn --loglevel {{ .Values.logLevel }} --daemon start resourcemanager 
      $HADOOP_HOME/bin/yarn --loglevel {{ .Values.logLevel }} --daemon start proxyserver
    fi

    # ------------------------------------------------------
    # Start NODE MANAGER
    # ------------------------------------------------------
    if [[ "${HOSTNAME}" =~ "yarn-nm" ]]; then
      sed -i '/<\/configuration>/d' $HADOOP_HOME/etc/hadoop/yarn-site.xml
      cat >> $HADOOP_HOME/etc/hadoop/yarn-site.xml <<- EOM
      <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>${MY_MEM_LIMIT:-2048}</value>
      </property>

      <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>${MY_CPU_LIMIT:-2}</value>
      </property>
    EOM

      echo '</configuration>' >> $HADOOP_HOME/etc/hadoop/yarn-site.xml

      # Wait with timeout for resourcemanager
      TMP_URL="http://{{ include "hadoop.fullname" . }}-yarn-rm:8088/ws/v1/cluster/info"
      if timeout 5m bash -c "until curl -sf $TMP_URL; do echo Waiting for $TMP_URL; sleep 5; done"; then
        $HADOOP_HOME/bin/yarn nodemanager --loglevel {{ .Values.logLevel }}
      else 
        echo "$0: Timeout waiting for $TMP_URL, exiting."
        exit 1
      fi

    fi

    # ------------------------------------------------------
    # Tail logfiles for daemonized workloads (parameter -d)
    # ------------------------------------------------------
    if [[ $1 == "-d" ]]; then
      until find ${HADOOP_HOME}/logs -mmin -1 | egrep -q '.*'; echo "`date`: Waiting for logs..." ; do sleep 2 ; done
      tail -F ${HADOOP_HOME}/logs/* &
      while true; do sleep 1000; done
    fi

    # ------------------------------------------------------
    # Start bash if requested (parameter -bash)
    # ------------------------------------------------------
    if [[ $1 == "-bash" ]]; then
      /bin/bash
    fi

  core-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
      <property>
            <name>fs.defaultFS</name>
            <value>hdfs://{{ include "hadoop.fullname" . }}-hdfs-nn:9000/</value>
            <description>NameNode URI</description>
        </property>
    </configuration>

  hdfs-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
    
{{- if .Values.hdfs.webhdfs.enabled -}}
      <property>
          <name>dfs.webhdfs.enabled</name>
          <value>true</value>
      </property> 
{{- end -}}

      <property>
        <name>dfs.datanode.use.datanode.hostname</name>
        <value>false</value>
      </property>

      <property>
        <name>dfs.client.use.datanode.hostname</name>
        <value>false</value>
      </property>

      <property>
        <name>dfs.datanode.hostname</name>
        <value>{{ .Values.hdfs.dataNode.externalHostname }}</value>
      </property>

      <property>
        <name>dfs.datanode.http.address</name>
        <value>0.0.0.0:EXTERNAL_HTTP_PORT_REPLACEME</value>
      </property>

      <property>
        <name>dfs.datanode.address</name>
        <value>0.0.0.0:EXTERNAL_DATA_PORT_REPLACEME</value>
      </property>

      <property>
        <name>dfs.replication</name>
          <value>3</value>
      </property>

      <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///root/hdfs/datanode</value>
        <description>DataNode directory</description>
      </property>

      <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///root/hdfs/namenode</value>
        <description>NameNode directory for namespace and transaction logs storage.</description>
      </property>

      <property>
        <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
        <value>false</value>
      </property>

      <!-- Bind to all interfaces -->
      <property>
        <name>dfs.namenode.rpc-bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>dfs.namenode.servicerpc-bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <!-- /Bind to all interfaces -->

    </configuration>

  mapred-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

    <configuration>
      <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
      </property>
      <property>
        <name>mapreduce.jobhistory.address</name>
        <value>{{ include "hadoop.fullname" . }}-yarn-rm-0.{{ include "hadoop.fullname" . }}-yarn-rm.{{ .Release.Namespace }}.svc.cluster.local:10020</value>
      </property>
      <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>{{ include "hadoop.fullname" . }}-yarn-rm-0.{{ include "hadoop.fullname" . }}-yarn-rm.{{ .Release.Namespace }}.svc.cluster.local:19888</value>
      </property>
    </configuration>

  capacity-scheduler.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

    <configuration>

      <property>
        <name>yarn.scheduler.capacity.maximum-applications</name>
        <value>10000</value>
        <description>
          Maximum number of applications that can be pending and running.
        </description>
      </property>

      <property>
        <name>yarn.scheduler.capacity.maximum-am-resource-percent</name>
        <value>0.1</value>
        <description>
          Maximum percent of resources in the cluster which can be used to run
          application masters i.e. controls number of concurrent running
          applications.
        </description>
      </property>

      <property>
        <name>yarn.scheduler.capacity.resource-calculator</name>
        <value>org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator</value>
        <description>
          The ResourceCalculator implementation to be used to compare
          Resources in the scheduler.
          The default i.e. DefaultResourceCalculator only uses Memory while
          DominantResourceCalculator uses dominant-resource to compare
          multi-dimensional resources such as Memory, CPU etc.
        </description>
      </property>

      <property>
        <name>yarn.scheduler.capacity.root.queues</name>
        <value>dev,prod</value>
        <description>
          The queues at the this level (root is the root queue).
        </description>
      </property>
      <property>
          <name>yarn.scheduler.capacity.root.dev.queues</name>
          <value>dev1,dev2</value>
      </property>
      <property>
          <name>yarn.scheduler.capacity.root.dev.capacity</name>
          <value>50</value>
          <description>Default queue target capacity.</description>
      </property>
      <property>
          <name>yarn.scheduler.capacity.root.dev.maximum-capacity</name>
          <value>50</value>
          <description>配置 root.default 队列可使用的资源上限</description>
      </property>
      <property>
          <name>yarn.scheduler.capacity.root.dev.dev1.capacity</name>
          <value>50</value>
      </property>
      <property>
          <name>yarn.scheduler.capacity.root.dev.dev2.capacity</name>
          <value>50</value>
      </property>
      <property>
          <name>yarn.scheduler.capacity.root.prod.queues</name>
          <value>prod1,prod2</value>
      </property>
      <property>
        <name>yarn.scheduler.capacity.root.prod.capacity</name>
        <value>50</value>
        <description>Default queue target capacity.</description>
      </property>
      <property>
        <name>yarn.scheduler.capacity.root.prod.maximum-capacity</name>
        <value>80</value>
      </property>
      <property>
          <name>yarn.scheduler.capacity.root.prod.prod1.capacity</name>
          <value>50</value>
      </property>
      <property>
          <name>yarn.scheduler.capacity.root.prod.prod2.capacity</name>
          <value>50</value>
      </property>

      <property>
        <name>yarn.scheduler.capacity.root.default.user-limit-factor</name>
        <value>1</value>
        <description>
          Default queue user limit a percentage from 0.0 to 1.0.
        </description>
      </property>

      <property>
        <name>yarn.scheduler.capacity.root.default.maximum-capacity</name>
        <value>100</value>
        <description>
          The maximum capacity of the default queue.
        </description>
      </property>

      <property>
        <name>yarn.scheduler.capacity.root.default.state</name>
        <value>RUNNING</value>
        <description>
          The state of the default queue. State can be one of RUNNING or STOPPED.
        </description>
      </property>

      <property>
        <name>yarn.scheduler.capacity.root.default.acl_submit_applications</name>
        <value>*</value>
        <description>
          The ACL of who can submit jobs to the default queue.
        </description>
      </property>

      <property>
        <name>yarn.scheduler.capacity.root.default.acl_administer_queue</name>
        <value>*</value>
        <description>
          The ACL of who can administer jobs on the default queue.
        </description>
      </property>

      <property>
        <name>yarn.scheduler.capacity.root.default.acl_application_max_priority</name>
        <value>*</value>
        <description>
          The ACL of who can submit applications with configured priority.
          For e.g, [user={name} group={name} max_priority={priority} default_priority={priority}]
        </description>
      </property>

       <property>
         <name>yarn.scheduler.capacity.root.default.maximum-application-lifetime
         </name>
         <value>-1</value>
         <description>
            Maximum lifetime of an application which is submitted to a queue
            in seconds. Any value less than or equal to zero will be considered as
            disabled.
            This will be a hard time limit for all applications in this
            queue. If positive value is configured then any application submitted
            to this queue will be killed after exceeds the configured lifetime.
            User can also specify lifetime per application basis in
            application submission context. But user lifetime will be
            overridden if it exceeds queue maximum lifetime. It is point-in-time
            configuration.
            Note : Configuring too low value will result in killing application
            sooner. This feature is applicable only for leaf queue.
         </description>
       </property>

       <property>
         <name>yarn.scheduler.capacity.root.default.default-application-lifetime
         </name>
         <value>-1</value>
         <description>
            Default lifetime of an application which is submitted to a queue
            in seconds. Any value less than or equal to zero will be considered as
            disabled.
            If the user has not submitted application with lifetime value then this
            value will be taken. It is point-in-time configuration.
            Note : Default lifetime can't exceed maximum lifetime. This feature is
            applicable only for leaf queue.
         </description>
       </property>

      <property>
        <name>yarn.scheduler.capacity.node-locality-delay</name>
        <value>40</value>
        <description>
          Number of missed scheduling opportunities after which the CapacityScheduler
          attempts to schedule rack-local containers.
          When setting this parameter, the size of the cluster should be taken into account.
          We use 40 as the default value, which is approximately the number of nodes in one rack.
          Note, if this value is -1, the locality constraint in the container request
          will be ignored, which disables the delay scheduling.
        </description>
      </property>

      <property>
        <name>yarn.scheduler.capacity.rack-locality-additional-delay</name>
        <value>-1</value>
        <description>
          Number of additional missed scheduling opportunities over the node-locality-delay
          ones, after which the CapacityScheduler attempts to schedule off-switch containers,
          instead of rack-local ones.
          Example: with node-locality-delay=40 and rack-locality-delay=20, the scheduler will
          attempt rack-local assignments after 40 missed opportunities, and off-switch assignments
          after 40+20=60 missed opportunities.
          When setting this parameter, the size of the cluster should be taken into account.
          We use -1 as the default value, which disables this feature. In this case, the number
          of missed opportunities for assigning off-switch containers is calculated based on
          the number of containers and unique locations specified in the resource request,
          as well as the size of the cluster.
        </description>
      </property>

      <property>
        <name>yarn.scheduler.capacity.queue-mappings</name>
        <value></value>
        <description>
          A list of mappings that will be used to assign jobs to queues
          The syntax for this list is [u|g]:[name]:[queue_name][,next mapping]*
          Typically this list will be used to map users to queues,
          for example, u:%user:%user maps all users to queues with the same name
          as the user.
        </description>
      </property>

      <property>
        <name>yarn.scheduler.capacity.queue-mappings-override.enable</name>
        <value>false</value>
        <description>
          If a queue mapping is present, will it override the value specified
          by the user? This can be used by administrators to place jobs in queues
          that are different than the one specified by the user.
          The default is false.
        </description>
      </property>

      <property>
        <name>yarn.scheduler.capacity.per-node-heartbeat.maximum-offswitch-assignments</name>
        <value>1</value>
        <description>
          Controls the number of OFF_SWITCH assignments allowed
          during a node's heartbeat. Increasing this value can improve
          scheduling rate for OFF_SWITCH containers. Lower values reduce
          "clumping" of applications on particular nodes. The default is 1.
          Legal values are 1-MAX_INT. This config is refreshable.
        </description>
      </property>


      <property>
        <name>yarn.scheduler.capacity.application.fail-fast</name>
        <value>false</value>
        <description>
          Whether RM should fail during recovery if previous applications'
          queue is no longer valid.
        </description>
      </property>

      <property>
        <name>yarn.scheduler.capacity.workflow-priority-mappings</name>
        <value></value>
        <description>
          A list of mappings that will be used to override application priority.
          The syntax for this list is
          [workflowId]:[full_queue_name]:[priority][,next mapping]*
          where an application submitted (or mapped to) queue "full_queue_name"
          and workflowId "workflowId" (as specified in application submission
          context) will be given priority "priority".
        </description>
      </property>

      <property>
        <name>yarn.scheduler.capacity.workflow-priority-mappings-override.enable</name>
        <value>false</value>
        <description>
          If a priority mapping is present, will it override the value specified
          by the user? This can be used by administrators to give applications a
          priority that is different than the one specified by the user.
          The default is false.
        </description>
      </property>

    </configuration>
  slaves: |
    localhost
  yarn-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

    <configuration>
       <!-- task resource -->
      <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>500</value>
      </property>
      <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
          <value>8192</value>
      </property>
      <property>
        <name>yarn.scheduler.minimum-allocation-vcores</name>
        <value>1</value>
      </property>
      <property>
        <name>yarn.scheduler.maximum-allocation-vcores</name>
        <value>2</value>
      </property>
      <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>{{ include "hadoop.fullname" . }}-yarn-rm</value>
      </property>

      <!-- Bind to all interfaces -->
      <property>
        <name>yarn.resourcemanager.bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>yarn.nodemanager.bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>yarn.timeline-service.bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <!-- /Bind to all interfaces -->
      <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
      </property>

      <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
      </property>

      <property>
        <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
      </property>

      <property>
        <description>List of directories to store localized files in.</description>
        <name>yarn.nodemanager.local-dirs</name>
        <value>/var/lib/hadoop-yarn/cache/${user.name}/nm-local-dir</value>
      </property>

      <property>
        <description>Where to store container logs.</description>
        <name>yarn.nodemanager.log-dirs</name>
        <value>/var/log/hadoop-yarn/containers</value>
      </property>

      <property>
        <description>Where to aggregate logs to.</description>
        <name>yarn.nodemanager.remote-app-log-dir</name>
        <value>/var/log/hadoop-yarn/apps</value>
      </property>

      <property>
        <name>yarn.application.classpath</name>
        <value>
          /opt/hadoop/etc/hadoop,
          /opt/hadoop/share/hadoop/common/*,
          /opt/hadoop/share/hadoop/common/lib/*,
          /opt/hadoop/share/hadoop/hdfs/*,
          /opt/hadoop/share/hadoop/hdfs/lib/*,
          /opt/hadoop/share/hadoop/mapreduce/*,
          /opt/hadoop/share/hadoop/mapreduce/lib/*,
          /opt/hadoop/share/hadoop/yarn/*,
          /opt/hadoop/share/hadoop/yarn/lib/*
        </value>
      </property>
    </configuration>
